trading_days_per_year: 252
minutes_per_trading_day: 390
label_lookahead_bars: 1
embargo_days: 1
cpcv:
  n_groups: 12
  test_group_size: 2
  random_state: 7
  max_splits: 20
dsr_threshold: 0.05
go_no_go:
  sharpe_min: 0.8
  dsr_p_max: 0.05
  slippage_budget_max: 0.8
reality_checks:
  n_bootstrap: 1000
  block_len_bars: 78
trading_costs_bps: 0.0
rl_fast_smoke: false
rl_fast_overrides:
  train_timesteps: 2000
rl_defaults: &rl_defaults
  train_timesteps: 50000
  n_envs: 1
  seed: 42
  vecnormalize_obs: true
  vecnormalize_reward: true
  policy_kwargs: {}
  algo_kwargs: {}
strategies:
  - name: ma_fast
    type: moving_average
    params:
      fast: 10
      slow: 40
  - name: ma_medium
    type: moving_average
    params:
      fast: 20
      slow: 60
  - name: ma_slow
    type: moving_average
    params:
      fast: 40
      slow: 120
  - name: ES_PPO_LSTM
    type: rl_policy
    algo: RecurrentPPO
    policy: MlpLstmPolicy
    rl:
      <<: *rl_defaults
  - name: NQ_SAC
    type: rl_policy
    algo: SAC
    policy: MlpPolicy
    rl:
      <<: *rl_defaults
      train_timesteps: 30000
      seed: 123
      algo_kwargs:
        learning_rate: 0.0003
        buffer_size: 1000000
        learning_starts: 100
        batch_size: 256
        tau: 0.005
        gamma: 0.99
        train_freq: 1
        gradient_steps: 1
        ent_coef: auto
  - name: GC_TD3
    type: rl_policy
    algo: TD3
    policy: MlpPolicy
    rl:
      <<: *rl_defaults
      algo_kwargs:
        learning_rate: 0.0003
        buffer_size: 1000000
        learning_starts: 100
        batch_size: 256
        tau: 0.005
        gamma: 0.99
        train_freq: 1
        gradient_steps: 1
        policy_delay: 2
        target_policy_smoothing:
          noise_std: 0.2
          noise_clip: 0.5
  - name: ES_LogReg
    type: supervised
    model: logistic
    params:
      target_col: label
      C: 0.5
